{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zillow Compete\n",
    "## Predict housing prices using regression techniques\n",
    "\n",
    "This notebook is a Zillow Compete clone which uses Azure Machine Learning components throughout. This notebook covers:\n",
    "\n",
    "-   Workspace untilization\n",
    "-   Azure ML Compute provisioning\n",
    "-   Machine Learning Pipelines\n",
    "    -   Data Ingestions\n",
    "    -   Data Preparation\n",
    "    -   Data Transform\n",
    "    -   Train, Evaluate, and Register a Model\n",
    "-  ML Model Deployment\n",
    "-  CI/CD pipelines (if time permits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import ML Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace              # connect to workspace\n",
    "from azureml.core import Experiment             # connect/create experiments\n",
    "from azureml.core import ComputeTarget          # connect to compute\n",
    "from azureml.core import Environment            # manage e.g. Python environments\n",
    "from azureml.core import Datastore, Dataset     # work with data\n",
    "from azureml.core.model import Model            # work with model\n",
    "import mlflow                                   # work with mlflow\n",
    "\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "exp_name = \"Zillow-Regr-Exp\"\n",
    "exp = Experiment(ws, exp_name)\n",
    "mlflow.set_experiment(exp_name)\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Datastore, Blobstore and Filestore in the workspace\n",
    "In this step, we will define the datastore, blobstore and file store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve an existing datastore in the workspace by name\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Get the blob storage associated with the workspace\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_blob_store.upload_files(\n",
    "    [\"../data/train.csv\"],\n",
    "    target_path=\"train-dataset\",\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_blob_store.upload_files(\n",
    "    [\"../data/test.csv\"],\n",
    "    target_path=\"test-dataset\",\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "ws = Workspace.from_config()\n",
    "datastore = Datastore.get(ws, 'workspaceblobstore')\n",
    "if not 'house_prices_train' in ws.datasets.keys() :\n",
    "    zillow_housing_dataset = Dataset.Tabular.from_delimited_files([(datastore, '../data/train.csv')])\n",
    "    zillow_housing_dataset.register(workspace = ws,\n",
    "                                     name = 'house_prices_train',\n",
    "                                     description = 'housing training data',\n",
    "                                     create_new_version = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zillow_housing_dataset = Dataset.get_by_name(ws, 'house_prices_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_housing_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Environment\n",
    "Create a docker based environment with sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "conda_dep = CondaDependencies.create(\n",
    "        conda_packages=['pandas','scikit-learn'], \n",
    "        pip_packages=['azureml-sdk[automl,explain]', 'azureml-dataprep[fuse,pandas]'], \n",
    "        pin_sdk_version=False)\n",
    "\n",
    "myenv = Environment('Zillow-Regr-AutoML')\n",
    "myenv.python.conda_dependencies = conda_dep\n",
    "\n",
    "# Enable Docker\n",
    "docker_config = DockerConfiguration(use_docker=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provision a Compute Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from socket import timeout\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cluster_name = \"train-clu\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D14',\n",
    "                                                            vm_priority='lowpriority',\n",
    "                                                            min_nodes= 0,\n",
    "                                                            max_nodes=4,\n",
    "                                                            idle_seconds_before_scaledown=120)\n",
    "                                                            \n",
    "    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "cluster.wait_for_completion(show_output=True, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the training run\n",
    "Make sure remote training cluster has all the dependencies that are required by the training steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "aml_run_config = RunConfiguration()\n",
    "# Use just-specified compute target (\"cpu-cluster\")\n",
    "aml_run_config.target = cluster\n",
    "\n",
    "USE_CURATED_ENV = False\n",
    "if USE_CURATED_ENV :\n",
    "    curated_environment = Environment.get(workspace=ws, name=\"Zillow-AutoML-Env\")\n",
    "    aml_run_config.environment = curated_environment\n",
    "else:\n",
    "    aml_run_config.environment.python.user_managed_dependencies = False\n",
    "    \n",
    "    # Add some packages relied on by data prep step\n",
    "    aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "        conda_packages=['pandas','scikit-learn'], \n",
    "        pip_packages=['azureml-sdk[automl,explain]', 'azureml-dataprep[fuse,pandas]'], \n",
    "        pin_sdk_version=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for Auto ML regression\n",
    "In this step, we are doing data preparation to drop columns that wont be used for prediction. This can be extended further to do complete data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create a folder for pipeline step files\n",
    "aml_dir = \"../scripts/\"\n",
    "os.makedirs(aml_dir, exist_ok=True)\n",
    "print(aml_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the first script, which will read data from the zillow housing tabular dataset and apply some simple pre-processing to remove any rows with missing data and normalize the numeric features so they're on a similar scale.\n",
    "\n",
    "The script includes a argument named *--output-path* which references the folder where the resulting data should be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $aml_dir/dataprep.py\n",
    "from azureml.core import Run\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import argparse\n",
    "from azureml.core import Run\n",
    "import mlflow\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "# Constants\n",
    "RANDOM_SEED=42\n",
    "\n",
    "# Get the parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--output_path', dest='output_path', required=True)\n",
    "args = parser.parse_args()\n",
    "zillow_housing_dataset = Run.get_context().input_datasets['house_prices_train']\n",
    "\n",
    "# Get the experiment/job run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Load the data (passed as an input dataset)\n",
    "df_train = zillow_housing_dataset.to_pandas_dataframe()\n",
    "\n",
    "# Save the prepped data\n",
    "print('Saving Data...')\n",
    "os.makedirs(os.path.dirname(args.output_path), exist_ok=True)\n",
    "pq.write_table(pa.Table.from_pandas(df_train), args.output_path)\n",
    "\n",
    "# End the run\n",
    "print(f\"Wrote test to {args.output_path} and train to {args.output_path}\")\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data preparation step for pipeline\n",
    "In this step we are defining data preparation step with the python file created earlier for data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData, Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "prepped_data_path = PipelineData(\"house_prices_train\",datastore,\"direct\").as_dataset()\n",
    "\n",
    "dataprep_step = PythonScriptStep(\n",
    "    name=\"dataprep_step\",\n",
    "    source_directory=aml_dir, \n",
    "    script_name=\"dataprep.py\", \n",
    "    compute_target=cluster, \n",
    "    runconfig=aml_run_config,\n",
    "    arguments=[\"--output_path\", prepped_data_path],\n",
    "    inputs=[zillow_housing_dataset.as_named_input(\"house_prices_train\")],\n",
    "    outputs=[prepped_data_path],\n",
    "    allow_reuse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send data to AutoML Step\n",
    "The snippet below creates a high-performing PipelineOutputTabularDataset from the PipelineOutputFileDataset output of the data preparation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped_train_data = prepped_data_path.parse_parquet_files(file_extension=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Automated ML Outputs\n",
    "The outputs of the AutoMLStep are the final metric scores of the higher-performing model and that model itself. To use these outputs in further pipeline steps, prepare PipelineData objects to receive them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import TrainingOutput\n",
    "\n",
    "metrics_data = PipelineData(name='metrics_data',\n",
    "                           datastore=datastore,\n",
    "                           pipeline_output_name='metrics_output',\n",
    "                           training_output=TrainingOutput(type='Metrics'))\n",
    "model_data = PipelineData(name='best_model_data',\n",
    "                           datastore=datastore,\n",
    "                           pipeline_output_name='model_output',\n",
    "                           training_output=TrainingOutput(type='Model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Create Automated ML Pipeline Step\n",
    "Once the inputs and outputs are defined, it's time to create the AutoMLConfig and AutoMLStep. The details of the configuration will depend on your task, in this case, it is regression to predict the 'Sales Price' label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "import logging\n",
    "\n",
    "automl_settings = {\n",
    "       \"n_cross_validations\":5,\n",
    "    #    \"primary_metric\": 'r2_score',\n",
    "       \"primary_metric\": 'normalized_root_mean_squared_error',\n",
    "      #  \"primary_metric\":   'normalized_mean_absolute_error',\n",
    "       \"enable_early_stopping\": True,\n",
    "       \"experiment_timeout_hours\": 1,\n",
    "       \"max_concurrent_iterations\": 4,\n",
    "       \"max_cores_per_iteration\": -1,\n",
    "       \"verbosity\": logging.INFO\n",
    "   }\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                               path = aml_dir,\n",
    "                               compute_target = cluster,\n",
    "                               training_data = prepped_train_data,\n",
    "                               featurization = 'auto',\n",
    "                               debug_log = 'automated_ml_errors.log',\n",
    "                               label_column_name = 'SalePrice',\n",
    "                               **automl_settings\n",
    "                               )\n",
    "\n",
    "train_step = AutoMLStep(name='automl_training_step',\n",
    "    automl_config=automl_config,\n",
    "    passthru_automl_config=False,\n",
    "    outputs=[metrics_data,model_data],\n",
    "    allow_reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model created by automated ML\n",
    "The last step in a basic ML pipeline is registering the created model. By adding the model to the workspace's model registry, it will be available in the portal and can be versioned. To register the model, write another PythonScriptStep that takes the model_data output of the AutoMLStep(first and the second cell below this cell performs these steps).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $aml_dir/register_model.py\n",
    "from azureml.core.model import Model, Dataset\n",
    "from azureml.core.run import Run, _OfflineRun\n",
    "from azureml.core import Workspace\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model_name\", required=True)\n",
    "parser.add_argument(\"--model_path\", required=True)\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f\"model_name : {args.model_name}\")\n",
    "print(f\"model_path: {args.model_path}\")\n",
    "\n",
    "run = Run.get_context()\n",
    "ws = Workspace.from_config() if type(run) == _OfflineRun else run.experiment.workspace\n",
    "\n",
    "model = Model.register(workspace=ws,\n",
    "                       model_path=args.model_path,\n",
    "                       model_name=args.model_name)\n",
    "\n",
    "print(\"Registered version {0} of model {1}\".format(model.version, model.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "# The model name with which to register the trained model in the workspace.\n",
    "model_name = PipelineParameter(\"model_name\", default_value=\"Zillow-SalesPrices-Regr-mdl\")\n",
    "\n",
    "register_step = PythonScriptStep(script_name=\"register_model.py\",\n",
    "                                       name=\"register_model_step\",\n",
    "                                       source_directory=aml_dir,\n",
    "                                       allow_reuse=True,\n",
    "                                       arguments=[\"--model_name\", model_name, \"--model_path\", model_data],\n",
    "                                       inputs=[model_data],\n",
    "                                       compute_target=cluster,\n",
    "                                       runconfig=aml_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and run the automated ML pipeline\n",
    "Creating and running a pipeline that contains the AutoML Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Workspace,Experiment,Run\n",
    "from azureml.widgets import RunDetails\n",
    "azureml._restclient.snapshots_client.SNAPSHOT_MAX_SIZE_BYTES = 20000000000\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline = Pipeline(ws, [dataprep_step, train_step, register_step])\n",
    "print('Pipeline is built.')\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name='zillow-regr-automl-pipeline')\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "RunDetails(pipeline_run).show()\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the pipeline has finished, you can examine the metrics recorded by it's child runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in pipeline_run.get_children():\n",
    "    print(run.name, ':')\n",
    "    metrics = run.get_metrics()\n",
    "    for metric_name in metrics:\n",
    "        print('\\t',metric_name, \":\", metrics[metric_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing entry script\n",
    "Write the entry script that will be used to predict on my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $aml_dir/score.py\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import azureml.automl.core\n",
    "from azureml.automl.core.shared import logging_utilities, log_server\n",
    "from azureml.telemetry import INSTRUMENTATION_KEY\n",
    "\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n",
    "from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n",
    "\n",
    "data_sample = PandasParameterType(pd.DataFrame({\"Id\": pd.Series([0], dtype=\"int64\"), \"MSSubClass\": pd.Series([0], dtype=\"int64\"), \"MSZoning\": pd.Series([\"example_value\"], dtype=\"object\"), \"LotFrontage\": pd.Series([\"example_value\"], dtype=\"object\"), \"LotArea\": pd.Series([0], dtype=\"int64\"), \"Street\": pd.Series([\"example_value\"], dtype=\"object\"), \"Alley\": pd.Series([\"example_value\"], dtype=\"object\"), \"LotShape\": pd.Series([\"example_value\"], dtype=\"object\"), \"LandContour\": pd.Series([\"example_value\"], dtype=\"object\"), \"Utilities\": pd.Series([\"example_value\"], dtype=\"object\"), \"LotConfig\": pd.Series([\"example_value\"], dtype=\"object\"), \"LandSlope\": pd.Series([\"example_value\"], dtype=\"object\"), \"Neighborhood\": pd.Series([\"example_value\"], dtype=\"object\"), \"Condition1\": pd.Series([\"example_value\"], dtype=\"object\"), \"Condition2\": pd.Series([\"example_value\"], dtype=\"object\"), \"BldgType\": pd.Series([\"example_value\"], dtype=\"object\"), \"HouseStyle\": pd.Series([\"example_value\"], dtype=\"object\"), \"OverallQual\": pd.Series([0], dtype=\"int64\"), \"OverallCond\": pd.Series([0], dtype=\"int64\"), \"YearBuilt\": pd.Series([0], dtype=\"int64\"), \"YearRemodAdd\": pd.Series([0], dtype=\"int64\"), \"RoofStyle\": pd.Series([\"example_value\"], dtype=\"object\"), \"RoofMatl\": pd.Series([\"example_value\"], dtype=\"object\"), \"Exterior1st\": pd.Series([\"example_value\"], dtype=\"object\"), \"Exterior2nd\": pd.Series([\"example_value\"], dtype=\"object\"), \"MasVnrType\": pd.Series([\"example_value\"], dtype=\"object\"), \"MasVnrArea\": pd.Series([0.0], dtype=\"float64\"), \"ExterQual\": pd.Series([\"example_value\"], dtype=\"object\"), \"ExterCond\": pd.Series([\"example_value\"], dtype=\"object\"), \"Foundation\": pd.Series([\"example_value\"], dtype=\"object\"), \"BsmtQual\": pd.Series([\"example_value\"], dtype=\"object\"), \"BsmtCond\": pd.Series([\"example_value\"], dtype=\"object\"), \"BsmtExposure\": pd.Series([\"example_value\"], dtype=\"object\"), \"BsmtFinType1\": pd.Series([\"example_value\"], dtype=\"object\"), \"BsmtFinSF1\": pd.Series([0], dtype=\"int64\"), \"BsmtFinType2\": pd.Series([\"example_value\"], dtype=\"object\"), \"BsmtFinSF2\": pd.Series([0], dtype=\"int64\"), \"BsmtUnfSF\": pd.Series([0], dtype=\"int64\"), \"TotalBsmtSF\": pd.Series([0], dtype=\"int64\"), \"Heating\": pd.Series([\"example_value\"], dtype=\"object\"), \"HeatingQC\": pd.Series([\"example_value\"], dtype=\"object\"), \"CentralAir\": pd.Series([False], dtype=\"bool\"), \"Electrical\": pd.Series([\"example_value\"], dtype=\"object\"), \"1stFlrSF\": pd.Series([0], dtype=\"int64\"), \"2ndFlrSF\": pd.Series([0], dtype=\"int64\"), \"LowQualFinSF\": pd.Series([0], dtype=\"int64\"), \"GrLivArea\": pd.Series([0], dtype=\"int64\"), \"BsmtFullBath\": pd.Series([0], dtype=\"int64\"), \"BsmtHalfBath\": pd.Series([0], dtype=\"int64\"), \"FullBath\": pd.Series([0], dtype=\"int64\"), \"HalfBath\": pd.Series([0], dtype=\"int64\"), \"BedroomAbvGr\": pd.Series([0], dtype=\"int64\"), \"KitchenAbvGr\": pd.Series([0], dtype=\"int64\"), \"KitchenQual\": pd.Series([\"example_value\"], dtype=\"object\"), \"TotRmsAbvGrd\": pd.Series([0], dtype=\"int64\"), \"Functional\": pd.Series([\"example_value\"], dtype=\"object\"), \"Fireplaces\": pd.Series([0], dtype=\"int64\"), \"FireplaceQu\": pd.Series([\"example_value\"], dtype=\"object\"), \"GarageType\": pd.Series([\"example_value\"], dtype=\"object\"), \"GarageYrBlt\": pd.Series([\"example_value\"], dtype=\"object\"), \"GarageFinish\": pd.Series([\"example_value\"], dtype=\"object\"), \"GarageCars\": pd.Series([0], dtype=\"int64\"), \"GarageArea\": pd.Series([0], dtype=\"int64\"), \"GarageQual\": pd.Series([\"example_value\"], dtype=\"object\"), \"GarageCond\": pd.Series([\"example_value\"], dtype=\"object\"), \"PavedDrive\": pd.Series([\"example_value\"], dtype=\"object\"), \"WoodDeckSF\": pd.Series([0], dtype=\"int64\"), \"OpenPorchSF\": pd.Series([0], dtype=\"int64\"), \"EnclosedPorch\": pd.Series([0], dtype=\"int64\"), \"3SsnPorch\": pd.Series([0], dtype=\"int64\"), \"ScreenPorch\": pd.Series([0], dtype=\"int64\"), \"PoolArea\": pd.Series([0], dtype=\"int64\"), \"PoolQC\": pd.Series([\"example_value\"], dtype=\"object\"), \"Fence\": pd.Series([\"example_value\"], dtype=\"object\"), \"MiscFeature\": pd.Series([\"example_value\"], dtype=\"object\"), \"MiscVal\": pd.Series([0], dtype=\"int64\"), \"MoSold\": pd.Series([0], dtype=\"int64\"), \"YrSold\": pd.Series([0], dtype=\"int64\"), \"SaleType\": pd.Series([\"example_value\"], dtype=\"object\"), \"SaleCondition\": pd.Series([\"example_value\"], dtype=\"object\")}))\n",
    "input_sample = StandardPythonParameterType({'data': data_sample})\n",
    "\n",
    "result_sample = NumpyParameterType(np.array([0]))\n",
    "output_sample = StandardPythonParameterType({'Results':result_sample})\n",
    "sample_global_parameters = StandardPythonParameterType(1.0)\n",
    "\n",
    "try:\n",
    "    log_server.enable_telemetry(INSTRUMENTATION_KEY)\n",
    "    log_server.set_verbosity('INFO')\n",
    "    logger = logging.getLogger('azureml.automl.core.scoring_script_v2')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # This name is model.id of model that we want to deploy deserialize the model file back\n",
    "    # into a sklearn model\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pkl')\n",
    "    path = os.path.normpath(model_path)\n",
    "    path_split = path.split(os.sep)\n",
    "    log_server.update_custom_dimensions({'model_name': path_split[-3], 'model_version': path_split[-2]})\n",
    "    try:\n",
    "        logger.info(\"Loading model from path.\")\n",
    "        model = joblib.load(model_path)\n",
    "        logger.info(\"Loading successful.\")\n",
    "    except Exception as e:\n",
    "        logging_utilities.log_traceback(e, logger)\n",
    "        raise\n",
    "\n",
    "@input_schema('Inputs', input_sample)\n",
    "@input_schema('GlobalParameters', sample_global_parameters, convert_to_provided_type=False)\n",
    "@output_schema(output_sample)\n",
    "def run(Inputs, GlobalParameters=1.0):\n",
    "    data = Inputs['data']\n",
    "    result = model.predict(data)\n",
    "    return {'Results':result.tolist()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the InferenceConfig\n",
    "Create the inference config that will be used when deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from logging.config import _RootLoggerConfiguration\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inf_config = InferenceConfig(entry_script='../scripts/score.py', environment=myenv, runtime='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provision the AKS cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your AKS cluster\n",
    "aks_name = 'aks-clu' \n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # Use the default configuration (can also provide parameters to customize)\n",
    "    prov_config = AksCompute.provisioning_configuration()\n",
    "\n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                    name = aks_name, \n",
    "                                    provisioning_configuration = prov_config)\n",
    "\n",
    "if aks_target.get_status() != \"Succeeded\":\n",
    "    aks_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy web service to AKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the web service configuration\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "\n",
    "aks_config = AksWebservice.deploy_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(ws, 'Zillow-SalesPrices-Regr-mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_service_name ='zillow-alpha-service'\n",
    "\n",
    "aks_service = Model.deploy(workspace=ws,\n",
    "                           name=aks_service_name,\n",
    "                           models=[model],\n",
    "                           inference_config=inf_config,\n",
    "                           deployment_config=aks_config,\n",
    "                           deployment_target=aks_target)\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the web service using the run method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Dataset.get_by_name(ws, 'house_prices_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "test_sample = json.dumps(test_data[1:2].to_list())\n",
    "test_sample = bytes(test_sample,encoding = 'utf8')\n",
    "\n",
    "prediction = aks_service.run(input_data = test_sample)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd2f20a2ae7e9e927b52643942994f3aab4e8a0fff0d99512b6bf37211656242"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
